[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Preface\nOrchestration, or the routine scheduling and exection of dependent tasks, is a core component of modern data work. Orchestration continues to reach more and more data workers - it was originally a focus for data engineers, but it now permeates the work of data analysts, analytics engineers, data scientists, and machine learning engineers. The easier it is for any class of data worker to orchestrate their code, the easier it is for any member of an organization to derive value from the outputs of that code."
  },
  {
    "objectID": "index.html#flavors-of-orchestration-code",
    "href": "index.html#flavors-of-orchestration-code",
    "title": "",
    "section": "Flavors of Orchestration Code",
    "text": "Flavors of Orchestration Code\nOrchestration with Python is a vast and opinionated landscape, but there are three clear flavors of orchestration to have emerged over time:\n\nObject-oriented orchestration, where tasks are objects and dependencies between tasks are handled with methods. Airflow’s classic style is a good example of object-oriented orchestration.\nDecorative orchestration, where tasks are functions and decorators are used to configure the tasks. Dependencies are often managed by passing the output of one function into the input of another. Airflow’s taskflow API and Dagster’s entire API are good examples of decorative orchestation.\nFile-oriented orchestration, where tasks are files, and dependencies are cleverly inferred or declared explicitly. Tools like Mage, dbt, and Orchest exemplify file-oriented orchestration."
  },
  {
    "objectID": "index.html#what-is-gusty",
    "href": "index.html#what-is-gusty",
    "title": "",
    "section": "What is gusty?",
    "text": "What is gusty?\ngusty is a file-oriented framework for Airflow, the absolute standard for orchestrators today. Airflow is a Top-Level Apache Project with sustained development, a gigantic ecosystem of provider packages, and is offered as a hosted service by major public clouds and other Airflow-focused companies. While other orchestrators natively support file-oriented orchestration, Airflow is such a good orchestrator that it was compelling to create a file-oriented framework for it. If you are reading this, you are likely already familiar with - or using - Airflow.\ngusty exists to make file-oriented orchestration fun and easy using Airflow, allowing for file-oriented DAGs to be incorporated in existing Airflow projects without any need to change existing work or Airflow code. You can use any Airflow operator with gusty. This document hopes to serve as a guide for getting the most out of file-oriented orchestration in Airflow using gusty."
  },
  {
    "objectID": "gusty-basics.html#gusty-dag-structure",
    "href": "gusty-basics.html#gusty-dag-structure",
    "title": "1  Basic DAG Structure",
    "section": "1.1 gusty DAG Structure",
    "text": "1.1 gusty DAG Structure\nA gusty DAG lives inside of your Airflow DAGs folder (by default $AIRFLOW_HOME/dags), and is comprised of a few core elements:\n\nTask Definition Files - Each file hold specifications for a given task. In the example below, hi.py, hey.sql, and hello.yml are our Task Definition Files. These Task Definition Files are all stored inside our hello_dag folder.\nMETADATA.yml - This optional file contains any argument that could be passed to Airflow’s DAG object, as well as some optional gusty-specifc argument. In the example below, METADATA.yml is stored inside of our hello_dag folder, alongside the Task Definition Files.\nDAG File - The file that turns a gusty DAG folder into an Airflow DAG. It’s more or less like any other Airflow DAG file, and it will contain gusty’s create_dag function. In the example below, hello_dag.py is our DAG Generation File. The DAG Generation File does not need to be named identically to the DAG folder.\n\n$AIRFLOW_HOME/dags/\n│\n├── hello_dag/\n│   ├── METADATA.yml\n│   ├── hi.py\n│   ├── hey.sql\n│   └── hello.yml\n│\n└── hello_dag.py\nIn the event you wanted to create a second gusty DAG, you can just repeat this pattern. For example, if we wanted to add goodbye_dag:\n$AIRFLOW_HOME/dags/\n│\n├── goodbye_dag/\n│   ├── METADATA.yml\n│   ├── bye.py\n│   ├── later.sql\n│   └── goodbye.yml\n|\n├── hello_dag/\n│   ├── METADATA.yml\n│   ├── hi.py\n│   ├── hey.sql\n│   └── hello.yml\n│\n├── goodbye_dag.py\n└── hello_dag.py"
  },
  {
    "objectID": "gusty-basics.html#task-definition-files",
    "href": "gusty-basics.html#task-definition-files",
    "title": "1  Basic DAG Structure",
    "section": "1.2 Task Definition Files",
    "text": "1.2 Task Definition Files\nThe three primary file types used for Task Definition Files are Python, SQL, and YAML. gusty supports other file types, but these three are the most commonly used. The general pattern for Task Definition files is that they contain:\n\nFrontmatter - YAML which carries the specification and parameterization for the task. This can include which Airflow (or custom) operator to use, any keyword arguments to be passed to that operator, and any task dependencies the given task may have.\nBody - The primary contents of the task. For example, the Body of a SQL file is the SQL statement which will be executed; the body of a Python file can be the python_callable that will be ran by the operator. For YAML files, there is no Body because the whole Task Definition File is YAML.\n\ngusty will pass any argument that can be passed to the operator specified (as well as any BaseOperator arguments) to the operator. The specified operator should be a full path to that operator.\nThe file name of each Task Definition File will become the name of the Airflow task.\nLet’s explore these different file types by looking at the contents of these Task Definition Files in hello_dag.\n\nYAML Files with hello.yml\nHere are the contents of our hello.yml file:\noperator: airflow.operators.bash.BashOperator\nbash_command: echo hello\nThe resulting task would contain a BashOperator with the task id hello.\nBecause the entire file is YAML, there is no separation of Frontmatter and Body.\n\n\nSQL Files with hey.sql\nHere are the contents of our hey.sql file:\n---\noperator: airflow.providers.sqlite.operators.sqlite.SqliteOperator\n---\n\nSELECT 'hey'\nThe resulting task would contain a SqliteOperator with the task id hey.\nThe Frontmatter of our SQL file is encased in a set of triple dashes (---). The Body of the file is everything below the second set of triple dashes. For SQL files, the Body of the file is passed to the sql argument of the underlying operator. In this case, SELECT 'hey' would be passed to the sql argument.\n\n\nPython Files with hi.py\nHere are the contents of our hi.py file:\n# ---\n# python_callable: say_hi\n# ---\n\ndef say_hi():\n  phrase = \"hi\"\n  print(phrase)\n  return phrase\nThe resulting task would contain a PythonOperator with the task id hi.\nThe Frontmatter of our Python file is also encased in a set of triple dashes (---), but you will also note that the entirety of the Frontmatter, including the triple dashes, are prefixed by comment hashes (#).\nBy default, gusty will specify specify Airflow’s PythonOperator as the operator, if no operator argument is provided. As with any Task Definition File, you can specify whatever operator is available to you in your Airflow environment, so you could just as easily add operator: airflow.operators.python.PythonVirtualenvOperator to this Frontmatter to use the PythonVirtualenvOperator instead of the PythonOperator.\nWhen a python_callable is specified in the Frontmatter of a Python file, gusty will search the Body of the Python file for a function with the name specified in the Frontmatter’s python_callable argument. For the best results with Python files, it’s recommended that you put all of the Body contents in a named function, as illustrated above."
  },
  {
    "objectID": "gusty-basics.html#metadata.yml",
    "href": "gusty-basics.html#metadata.yml",
    "title": "1  Basic DAG Structure",
    "section": "1.3 METADATA.yml",
    "text": "1.3 METADATA.yml\nThe METADATA.yml file is a special file for passing DAG-related arguments to Airflow’s DAG object. Airflow’s DAG object takes arguments like schedule (when you want your DAG to run), default_args.start_date (how far back you want your DAG to start), default_args.email (who should be notified if a task in DAG fails), and more. The METADATA.yml file is a convenient way to pass this information to Airflow.\nLet’s look at the contents of the METADATA.yml file in our hello_dag folder:\ndescription: \"Saying hello using different file types\"\ndoc_md: |-\n  This is a longform description,\n  which can be accessed from Airflow's\n  Graph view for your DAG. It looks\n  like a tiny poem.\nschedule: \"0 0 * * *\"\ncatchup: False\ndefault_args:\n    owner: You\n    email: you@you.com\n    start_date: !days_ago 28\n    email_on_failure: True\n    email_on_retry: False\n    retries: 1\n    retry_delay: !timedelta \n      minutes: 5\ntags:\n  - docs\n  - demo\n  - hello\nThe above METADATA.yml configures a DAG that runs once a day (schedule: \"0 0 * * *\"), has a start date of 28 days ago (default_args.start_date: !days_ago 28), and is tagged with the tags docs, demo, and hello. It also adds a description, a doc_md, and more, but every argument here is simply an argument in Airflow’s DAG object.\nThe only thing that you might not have seen before are YAML constructors, as illustrated above in the default_args.start_date (using !days_ago) and default_args.retry_delay (using !timedelta) arguments, which are calling functions inside of YAML. In short, YAML constructors are just Python functions that are called when your YAML (or any Task Definition File Frontmatter) is loaded. We’ll discuss YAML constructors more in later sections, but they are a powerful way to control File-oriented DAGs and tasks, and help ensure you have just as much control over your DAGs as writing them any other way.\nWe’ll also cover gusty-specific METADATA.yml later on, but for now, all you need to know is that the METADATA.yml file is used for passing arguments to Airflow’s DAG object."
  },
  {
    "objectID": "gusty-basics.html#dag-file",
    "href": "gusty-basics.html#dag-file",
    "title": "1  Basic DAG Structure",
    "section": "1.4 DAG File",
    "text": "1.4 DAG File\nFinally, let’s look at the DAG file that ultimately generates the Airflow DAG, hello_dag.py:\nimport os\nfrom gusty import create_dag\n\n# There are many different ways to find Airflow's DAGs directory.\n# hello_dag_dir returns something like: \"/usr/local/airflow/dags/hello_dag\"\nhello_dag_dir = os.path.join(\n  os.environ[\"AIRFLOW_HOME\"], \n  \"dags\", \n  \"hello_dag\")\n\nhello_dag = create_dag(hello_dag_dir, latest_only=False)\ngusty’s create_dag function takes as its first argument the path to a directory containing Task Definition Files, in our case the hello_dag direcory. Any keyword argument that can be passed to Airflow’s DAG object can be passed to create_dag, where any arguments that are specified both in create_dag and METADATA.yml will take the value specified in METADATA.yml.\nAdditionally, create_dag takes some gusty-specific arguments, one of which is illustrated here: latest_only=False, which disables gusty’s default behavior of installing a LatestOnlyOperator at the absolute root of an Airflow DAG. You can read more about the LatestOnlyOperator in Airflow’s documentation, but setting latest_only=False will ensure a gusty-generated DAG mirrors Airflow’s default behavior.\n\nIn subsequent chapters, we’ll cover more of gusty’s capabilities, but these are the core components of generating a file-oriented Airflow DAG with gusty!"
  },
  {
    "objectID": "task-dependencies.html#dependencies",
    "href": "task-dependencies.html#dependencies",
    "title": "2  Task Dependencies",
    "section": "2.1 Dependencies",
    "text": "2.1 Dependencies\nLet’s say that our hello task depended on our hi task running before it. To specify this dependency, we would add the hi task to a list in the dependencies block of the hello.yml Task Definition File:\noperator: airflow.operators.bash.BashOperator\ndependencies:\n  - hi\nbash_command: echo hello\nNow, in our Airflow UI, our DAG graph will show that hi precedes hello.\nRemember, in gusty, the file name (minus the file extension) becomes the task id, so you do not need to specify hi.py, just hi.\nYou can list as many dependencies as you need to for any task."
  },
  {
    "objectID": "task-dependencies.html#external-dependencies",
    "href": "task-dependencies.html#external-dependencies",
    "title": "2  Task Dependencies",
    "section": "2.2 External Dependencies",
    "text": "2.2 External Dependencies\nA common pattern in Airflow is to have tasks in one DAG depend on tasks in another DAG, or to have one DAG depend completely on another DAG. This behavior is possible in gusty by using the external_dependencies block. The external_dependencies block accepts a list of key-value pairs where each key is a DAG id and each value is a task id.\nFor each key-value pair listed in the external_dependencies block, gusty will generate an ExternalTaskSensor, a built-in Airflow sensor, and place the resulting sensor task upstream of the given dependent task. If the same external dependency is specified across multiple tasks, gusty will only create one sensor and place this one sensor upstream of all tasks with the specified external dependency.\nThere are a few ways to configure external dependencies, and we’ll look at all of them below.\n\nSingle Task External Dependency\nLet’s keep building up our hello.yml Task Definition File.\nTo specify that our hello task depends on an upstream task, which we’ll call upstream_task, in an upstream DAG, which we’ll call upstream_dag, we add the following external_dependencies block:\noperator: airflow.operators.bash.BashOperator\ndependencies:\n  - hi\nexternal_dependencies:\n  - upstream_dag: upstream_task\nbash_command: echo hello\nThe result will be a new ExternalTaskSensor task with the task id wait_for_upstream_dag_upstream_task, preceding the existing hello task.\nAs with dependencies, you can list as many external dependecies as you require.\n\n\nWhole DAG External Dependency\nAn alternative to speciying a single task for an external dependency is to specify that the entire upstream DAG is the dependency. In this case, we use the special keyword all to configure the ExternalTaskSensor to wait for the entire DAG:\noperator: airflow.operators.bash.BashOperator\ndependencies:\n  - hi\nexternal_dependencies:\n  - upstream_dag: all\nbash_command: echo hello\nThe result will be a new ExternalTaskSensor task with the task id wait_for_DAG_upstream_dag, preceding the existing hello task.\n\n\nExternal Dependencies in METADATA.yml\nAs an Airflow project grows, you might find that more and more of your tasks have the same external dependency, or sometimes DAGs just logically should depend on one another (e.g. a DAG that ingests data should precede a DAG that transforms that data). For these cases, you can utilize the same exact same external_dependencies block in any METADATA.yml file.\nWhen you specify an external dependency in a METADATA.yml file, the ExternalTaskSensor task will be placed at the root of the DAG, ensuring that no tasks in the DAG run before the ExternalTaskSensor task completes.\n\n\nOffset Schedules\nUnderstandably, but frustratingly, the default behavior of Airflow’s ExternalTaskSensor is to look for DAG runs that have that have ran at the same “logical date”. This means that if you have one DAG scheduled to run daily at 00:00 UTC (\"0 0 * * *\"), let’s call this DAG earlier_dag, and another DAG scheduled to run daily at 06:00 UTC (\"0 6 * * *\"), let’s call this DAG later_dag, and you specify an external dependency between later_dag and earlier_dag, the default syntax for an external_dependencies block will not work, because - in the case where later_dag depends on earlier_dag - the ExternalTaskSensor in later_dag will be looking for an 06:00 UTC DAG run of earlier_dag, which does not exist.\nFortunately, the external_dependencies block accepts an alternative syntax for this scenario, where:\n\nThe keys under external_dependencies are the external DAG ids.\nA tasks list is provided for a given external DAG.\nAdditional configuration for the ExternalTaskSensor class, such as the execution_delta, can be passed in.\n\nFor example, to configure later_dag (06:00 UTC) to depend on earlier_dag (00:00 UTC), we could add the following block to later_dag’s METADATA.yml:\nexternal_dependencies:\n  earlier_dag:\n    execution_delta: !timedelta\n      hours: 6\n    tasks:\n      - all\nThis will ensure the resulting wait_for_DAG_earlier_dag looks for a successful earlier_dag DAG run at 00:00 UTC (later_dag’s 06:00 UTC run minus 6 hours).\n\n\nAlternative Approaches to Offset Schedules\n\nCustom Sensors\nIt’s possible to create a custom sensor that “doesn’t care” about the logical date, and just looks at the last/latest DAG run. This ensures you don’t have to worry about setting any offset schedules.\nHere is a small snippet inspired by the cal-itp/data-infra repo (which they since deleted in this commit):\nfrom airflow.utils.db import provide_session\nfrom airflow.sensors.external_task_sensor import ExternalTaskSensor\n\n\nclass LastDagRunSensor(ExternalTaskSensor):\n    def __init__(self, external_dag_id, external_task_id=None, **kwargs):\n        super().__init__(\n          external_dag_id=external_dag_id, \n          external_task_id=external_task_id,\n          **kwargs)\n\n        def dag_last_exec(crnt_dttm):\n            return self.get_dag_last_execution_date(self.external_dag_id)\n\n        self.execution_date_fn = dag_last_exec\n\n    @provide_session\n    def get_dag_last_execution_date(self, dag_id, session):\n        from airflow.models import DagModel\n\n        q = session.query(DagModel).filter(DagModel.dag_id == self.external_dag_id)\n\n        dag = q.first()\n        return dag.get_last_dagrun().logical_date\nIn the event you wanted to use this LastDagRunSensor as the sensor class for the external dependencies in your gusty DAG, you could do so by using the wait_for_class argument available in create_dag. For example, here’s what your later_dag.py DAG file might look like if you decided to do so:\nimport os\nfrom gusty import create_dag\n# Wherever you store the code for the above sensor..\nfrom plugins.sensors import LastDagRunSensor\n\nlater_dag_dir = os.path.join(\n  os.environ[\"AIRFLOW_HOME\"], \n  \"dags\", \n  \"later_dag\")\n\nlater_dag = create_dag(\n  later_dag_dir, \n  wait_for_class=LastDagRunSensor,\n  latest_only=False)\nNow all of the external dependencies defined in the later_dag’s Task Definition Files will use the custom LastDagRunSensor instead of the default ExternalTaskSensor.\n\n\n\nOther External Dependency Considerations\nYou can configure your external dependencies further using the wait_for_defaults argument in create_dag, which accepts a dictionary of arguments that are available to Airflow’s ExternalTaskSensor. Here is the subset of parameters available in wait_for_defaults:\n\npoke_interval\ntimeout\nretries\nmode\nsoft_fail\nexecution_delta\nexecution_date_fn\ncheck_existence\n\nAdditionally, anything available to BaseOperator will be passed through.\n\nSet mode to reschedule\nBy default in Airflow, sensors run in mode=\"poke\", which means they take up a worker slot for the entire time they are waiting for the external task/DAG to complete. You can set mode=\"reschedule\" to free up the worker slot in between “pokes”. Building on the create_dag call in later_dag.py above:\nlater_dag = create_dag(\n  later_dag_dir, \n  wait_for_class=LastDagRunSensor,\n  wait_for_defaults={\n    \"mode\": \"reschedule\"\n    },\n  latest_only=False)\n\n\nSet a timeout\nBy default in gusty, external dependencies will timeout after 1 hour, or 3600 seconds. If you want to wait longer, you can set your timeout, in seconds:\nlater_dag = create_dag(\n  later_dag_dir, \n  wait_for_class=LastDagRunSensor,\n  wait_for_defaults={\n    \"mode\": \"reschedule\",\n    \"timeout\": 7200 # 2 hours in seconds\n    },\n  latest_only=False)\n\n\nLearn More\nIf you want to learn more about sensors, check out Airflow’s BaseSensorOperator and Airflow’s BaseOperator."
  },
  {
    "objectID": "task-groups.html#task-group-metadata.yml",
    "href": "task-groups.html#task-group-metadata.yml",
    "title": "3  Task Groups",
    "section": "3.1 Task Group METADATA.yml",
    "text": "3.1 Task Group METADATA.yml\nYou can add a METADATA.yml file to any task group folder. This is useful for when you want to have specific task group behavior, such as different default_args or if you want to prefix or suffix the task group id onto the task id.\nLet’s add a METADATA.yml file to our spanish task group subfolder:\ndags/\n│\n├── hello_dag/\n│   │\n│   ├── english/\n│   │   ├── hello.yml\n│   │   ├── hey.sql\n│   │   └── hi.py\n│   │\n│   ├── french/\n│   │   ├── bonjour.py\n│   │   ├── bonsoir.sql\n│   │   └── salut.yml\n│   │\n│   ├── spanish/\n│   │   ├── METADATA.yml\n│   │   ├── hola.py\n│   │   ├── oye.sql\n│   │   └── saludos.yml\n│   │\n│   └── METADATA.yml\n│\n└── hello_dag.py\nThe contents of this task group METADATA.yml file might look something like this:\ntooltip: \"This task group contains Spanish greetings.\"\nprefix_group_id: True\nThe above METADATA.yml will give the spanish task group a tooltip, when hoving over the node in the Airflow UI’s graph view, and all tasks in the task group will be prefixed with spanish_, such as spanish_oye and spanish_saludos.\nLastly, just like tasks, task group METADATA.yml can take advantage of dependencies blocks. So if a lot of tasks depend on the same upstream task, it might make sense to put them in the same task group folder, and set the upstream dependency in the METADATA.yml."
  },
  {
    "objectID": "task-groups.html#why-use-task-group-folders",
    "href": "task-groups.html#why-use-task-group-folders",
    "title": "3  Task Groups",
    "section": "3.2 Why Use Task Group Folders?",
    "text": "3.2 Why Use Task Group Folders?\nTask groups folders serve a few powerful purposes at scale:\n\nThey help keep Task Definition Files organized.\nThey can help keep parts of a DAG logically compartmentalized.\nThey can help keep dependencies between sets of tasks easier to manage."
  },
  {
    "objectID": "many-dags.html#using-create_dags",
    "href": "many-dags.html#using-create_dags",
    "title": "4  Many DAGs",
    "section": "4.1 Using create_dags",
    "text": "4.1 Using create_dags\nNow, we’ll use the create_dags function in gusty_dags.py to generate multiple DAGs in a single file! Here’s what our gusty_dags.py file looks like:\nimport os\nfrom gusty import create_dags\nfrom gusty.utils import days_ago\n\n# gusty_dags_dir returns something like: \"/usr/local/airflow/dags/gusty_dags\"\ngusty_dags_dir = os.path.join(\n  os.environ[\"AIRFLOW_HOME\"], \n  \"dags\", \n  \"gusty_dags\")\n\ncreate_dags(\n  gusty_dags_dir,\n  globals(),\n  schedule=\"0 0 * * *\",\n  catchup=False,\n  default_args={\n    \"owner\": \"you\",\n    \"email\": \"you@you.com\",\n    \"start_date\": days_ago(1)\n  },\n  wait_for_defaults={\n    \"mode\": \"reschedule\"\n  },\n  extra_tags=[\"gusty_dags\"],\n  latest_only=False)\nThe above will create both hello_dag and goodbye_dag DAGs, which reside inside of the gusty_dags_dir defined in gusty_dags.py.\nThe second argument, globals(), assigns the DAGs to the global environment, so Airflow can find the DAGs.\nschedule, catchup, default_args are arguments available in the Airflow DAG object.\nwait_for_defaults, extra_tags, and latest_only are all gusty-specific create_dag arguments. wait_for_defaults and latest_only were previously discussed here and here. extra_tags are additional tags appended to any existing tags specified in either create_dag or a METADATA.yml file."
  },
  {
    "objectID": "many-dags.html#the-power-of-create_dags",
    "href": "many-dags.html#the-power-of-create_dags",
    "title": "4  Many DAGs",
    "section": "4.2 The Power of create_dags",
    "text": "4.2 The Power of create_dags\nThe value in create_dags is that multiple DAGs can be created with common schedules, default arguments, tags, and more, plus each DAG can contain DAG-specific information, such as documentation (e.g. description and doc_md) and tags, inside their own METADATA.yml.\nIn gusty, METADATA.yml takes precedence over any create_dag argument, so you can override anything set in create_dags with the DAG-specific METADATA.yml.\n\nNow you have the building blocks to use file-oriented orchestration in Airflow with gusty!"
  },
  {
    "objectID": "using-constructors.html#what-are-constructors",
    "href": "using-constructors.html#what-are-constructors",
    "title": "5  Using Constructors",
    "section": "5.1 What are Constructors?",
    "text": "5.1 What are Constructors?\nConstructors are functions you can invoke in your YAML. These functions are invoked every time your Task Definition File is loaded during gusty’s DAG creation process.\nConstructors are available to us thanks the PyYAML package.\nTo better understand constructors, let’s orient ourselves around a simple Python function, called double_it:\ndef double_it(x):\n  return x + x\nIf we were to run double_it(2), we’d get back 4.\nTo invoke double_it from YAML, we begin our value entry with an exclaimation point (!), as illustrated below:\nsome_argument: !double_it 2\nWhen this YAML is loaded, the argument some_argument in our YAML will be assigned the value 4.\nYou can also use keyword arguments (i.e. double_it(x=2)) with constructors:\nsome_argument: !double_it\n  x: 2\nThe above will still result in some_argument taking on the value of 4."
  },
  {
    "objectID": "using-constructors.html#using-constructors-with-gusty",
    "href": "using-constructors.html#using-constructors-with-gusty",
    "title": "5  Using Constructors",
    "section": "5.2 Using Constructors with gusty",
    "text": "5.2 Using Constructors with gusty\ngusty makes it easy for you to leverage YAML contructors. The simplest way to leverage your functions as YAML constructors within gusty is to use the Airflow DAG object’s built-in user_defined_macros argument. When you pass a dictionary of functions/macros to user_defined_macros, gusty will make all of those functions/macros available to you as YAML constructors.\nYour call to create_dag might look something like this:\ncreate_dag(\n  ...,\n  user_defined_macros={\n    \"double_it\": double_it\n  }\n)\nThen, in a Task Definition File, you could leverage double_it both as a YAML constructor, as well as - just as in any other Airflow task - using Jinja. Here’s a BashOperator example below.\noperator: airflow.operators.bash.BashOperator\nretries: !double_it 4\nbash_command: echo {{ double_it(\"hello\") }}\nThe above would result in a task with 8 retries and a bash command that (when executed) would echo hellohello.\nAn important note on the timing of function evaluation: double_it is used twice above, once as a YAML constructor in the retries argument and once as a Jinja macro in the bash_command argument. The YAML constructor will be evaluated every time the DAG is generated, which is once every few minutes by default (in Airflow). The Jinja macro will only be evaluated when the task is executed."
  },
  {
    "objectID": "using-constructors.html#built-in-constructors",
    "href": "using-constructors.html#built-in-constructors",
    "title": "5  Using Constructors",
    "section": "5.3 Built-in Constructors",
    "text": "5.3 Built-in Constructors\n\ngusty\nThere are a few built-in constructors gusty contains, primarily to make creating a DAG using METADATA.yml easy. The three built-in constructors are datetime, timedelta, and days_ago, which simply provides a datetime object for as many days ago you specify.\n\n\nABSQL\nThe YAML loading functionality for gusty is maintained in a separate, lightweight project called ABSQL.\nThe ABSQL package ships with a handful of default functions, which are also available to you as both YAML constructors and macros within gusty DAGs."
  },
  {
    "objectID": "multi-tasking.html#multi_task_spec",
    "href": "multi-tasking.html#multi_task_spec",
    "title": "6  Multi-tasking",
    "section": "6.1 multi_task_spec",
    "text": "6.1 multi_task_spec\nImagine we want three BashOperator tasks to echo “hi”, “hey”, and “hello world”.\nWe can define all three tasks in a single Task Definition File, which we’ll call multi_greeing.yml. The name of the Task Definition File is arbitrary. Here are its contents:\noperator: airflow.operators.bash.BashOperator\nbash_command: echo $GREETING\nmulti_task_spec:\n  say_hi:\n    env:\n      GREETING: hi\n  say_hey:\n    env:\n      GREETING: hey\n  say_hello_world:\n    env:\n      GREETING: hello\n    bash_command: $GREETING world\nThe above Task Definition File will create three tasks: say_hi, say_hey, and say_hello_world. The tasks say_hi and say_hey both inherit the same bash_command, but have different env arguments. The say_hello_world task also contains its own env argument, but goes a step further as to define its own bash_command.\nThis powerful syntax allows you to keep your task definitions DRY. In this example, every task has a dedicated, unique env argument. In the case of say_hi and say_hey, they share a common bash_command. In the case of say_hello_world, it gets its own env and bash_command. Very flexible!"
  },
  {
    "objectID": "multi-tasking.html#python_callable_partials",
    "href": "multi-tasking.html#python_callable_partials",
    "title": "6  Multi-tasking",
    "section": "6.2 python_callable_partials",
    "text": "6.2 python_callable_partials\nSimilar to multi_task_spec, python_callable_partials allows you to generate multiple tasks in a single file, except instead of passing arguments to an operator, you pass arguments directly to a python_callable.\nIn the example Task Definition File below, we’ll create a few tasks to fetch the past year’s stock data from yfinance for three different stock symbols: AMZN, GOOG, and MSFT.\n# ---\n# python_callable: main\n# python_callable_partials:\n#   get_amzn:\n#     symbol: AMZN\n#   get_goog:\n#     symbol: GOOG\n#   get_msft:\n#     symbol: MSFT\n# ---\n\ndef main(symbol):\n  from yfinance import Ticker\n\n  stock = Ticker(symbol)\n  history = stock.history(period='1y', interval='1d').reset_index()\n  history[\"Symbol\"] = symbol\n  return history\nThe above Task Definition File will create three tasks: get_amzn, get_goog, and get_msft. Each task will have its respective symbol passed to the the main function."
  },
  {
    "objectID": "multi-tasking.html#mixing-it-up",
    "href": "multi-tasking.html#mixing-it-up",
    "title": "6  Multi-tasking",
    "section": "6.3 Mixing It Up",
    "text": "6.3 Mixing It Up\nmulti_task_spec and python_callable_partials are non-exclusive, so you can mix and match configuration as needed.\nLet’s build upon our yfinance example, and instead of using the default PythonOperator, let’s use the PythonVirtualenvOperator, and change the requirements for our get_amzn task.\n# ---\n# operator: airflow.operators.python.PythonVirtualenvOperator\n# python_callable: main\n# python_callable_partials:\n#   get_amzn:\n#     symbol: AMZN\n#   get_goog:\n#     symbol: GOOG\n#   get_msft:\n#     symbol: MSFT\n# multi_task_spec:\n#   get_amzn:\n#     requirements:\n#       - yfinance==0.1.96\n# ---\n\ndef main(symbol):\n  from yfinance import Ticker\n\n  stock = Ticker(symbol)\n  history = stock.history(period='1y', interval='1d').reset_index()\n  history[\"Symbol\"] = symbol\n  return history\nIn the above example, we changed two things:\n\nWe are now explicitly using the PythonVirtualenvOperator via the operator entry.\nOur get_amzn task also gets an entry in the multi_task_spec block, specifying a list of requirements just for our get_amzn task.\n\nWith both multi_task_spec and python_callable_partials working together, you can pretty much iterate over anything!"
  },
  {
    "objectID": "custom-operators.html#auto-detecting-dependencies",
    "href": "custom-operators.html#auto-detecting-dependencies",
    "title": "7  Custom Operators",
    "section": "7.1 Auto-detecting Dependencies",
    "text": "7.1 Auto-detecting Dependencies\n\nHow It Works\n\nIn gusty, the task name is the file name.\ngusty (optionally) makes available to your custom operators a task_id, which is the file name. Just specify task_id as an argument in the operator’s __init__ method, and gusty will pass it in when building your task.\nIf you name your SQL tables after the task_id, you can detect table names, which in turn can be used as a list of dependencies.\nIf you attach this list of dependencies as an attribute on your custom operator, gusty automatically wires up these dependencies for you.\n\nLet’s make an example custom SQL operator that takes advantage of this.\n\n\nExample Operator\nTo make our custom operator we will use:\n\nThe PostgresOperator from Airflow’s Postgres Provider, as the parent class for our custom operator.\nThe Parser from the sql-metadata package, for detecting table names.\n\nA common purpose of SQL tasks is to create tables, so we will have our users provide SELECT statements, and will wrap their statements in a CREATE OR REPLACE TABLE statement from within the operator.\nThis custom operator can be stored in our Airflow plugins folder, maybe under plugins/custom_operators/__init__.py. We’ll store a function for detecting tables, detect_tables, in this file, as well, for this example.\nfrom sql_metadata import Parser\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\n\n# ---------------------- #\n# Detect Tables Function #\n# ---------------------- #\n\ndef detect_tables(sql):\n  \"\"\"Detects tables in a sql query.\"\"\"\n  \n  # Remove any Jinja syntax to improve table detection\n  jinjaless_sql = sql.replace(\"{{\", \"\").replace(\"}}\", \"\")\n  \n  # Can return \"schema.table\", but we just want the \"table\"\n  tables_raw = Parser(jinjaless_sql).tables\n  \n  # Only take \"table\" if \"schema.table\" is in tables_raw\n  tables = [t.split('.')[-1] for t in tables_raw]\n  \n  return tables\n\n# --------------- #\n# Custom Operator #\n# --------------- #\n\nclass CustomPostgresOperator(PostgresOperator):\n\n    def __init__(\n            self,\n            # gusty automatically passes in task_id when creating the task\n            task_id,\n            schema,\n            sql,\n            postgres_conn_id = \"postgres_default\",\n            **kwargs):\n        \n        # gusty uses self.dependencies to create task dependencies\n        self.dependencies = detect_dependencies(sql)\n        \n        # Always name your table after the task_id / file name\n        table = task_id\n\n        create_sql = f\"CREATE OR REPLACE TABLE {schema}.{table} AS ({sql})\"\n\n        super(CustomPostgresOperator, self).__init__(\n            task_id = task_id,\n            sql = create_sql,\n            postgres_conn_id = postgres_conn_id,\n            **kwargs)\n\n\nExample Usage\n\nusers Table\nNow that we have our custom operator, we can invoke it in a Task Definition File. Let’s use this customer operator to create a users table, in a Task Definition File named users.sql.\n---\noperator: plugins.custom_operators.CustomPostgresOperator\nschema: app_data\n---\n\nSELECT\n  id AS user_id,\n  created_at\nFROM raw_data.users_raw\nPer our custom operator, this will create a users table in our app_data schema.\n\n\nnew_users Table\nNow we can make a second Task Definition File, new_users.sql, which references the users table.\n---\noperator: plugins.custom_operators.CustomPostgresOperator\nschema: app_data\n---\n\nSELECT\n  user_id\nFROM app_data.users\nWHERE DATE(created_at) = CURRENT_DATE()\nIn our Airflow DAG graph, the new_users task now automatically depends on the users task!"
  },
  {
    "objectID": "custom-operators.html#running-notebooks",
    "href": "custom-operators.html#running-notebooks",
    "title": "7  Custom Operators",
    "section": "7.2 Running Notebooks",
    "text": "7.2 Running Notebooks\n\nHow It Works\n\nJust like task_id is a special keyword you can add to your custom operator’s __init__ method, so is file_path.\nfile_path will be an absolute path to your Task Definition File, in this case a Jupyter Notebook.\nDeclare a YAML cell in your Jupyter Notebook, specifying the operator that should run the cell.\nHave your custom operator run the cell.\n\n\n\nExample Operator\nWe’ll more or less take this example directly from the gusty demo.\nTo make our custom operator we will use:\n\nThe built-in BashOperator, for running the command that renders the notebook.\nThe nbconvert package to render the notebook.\n\nOur operator will simply render the notebook as HTML and then delete it.\nfrom airflow.operators.bash_operator import BashOperator\n\ncommand_template = \"\"\"\njupyter nbconvert --to html --execute {file_path} || exit 1; rm -f {rendered_output}\n\"\"\"\n\nclass JupyterOperator(BashOperator):\n    \"\"\"\n    The LocalJupyterOperator executes the Jupyter Notebook.\n    Note that it is up to the notebook itself to handle connecting\n    to a database. (But it can grab this from Airflow connections)\n    \"\"\"\n\n    def __init__(\n      self,\n      # gusty automatically passes in file_path when creating the task\n      file_path,\n      *args,\n      **kwargs):\n      \n        self.file_path = file_path\n        self.rendered_output = self.file_path.replace('.ipynb', '.html')\n\n        command = command_template.format(file_path = self.file_path,\n                                          rendered_output = self.rendered_output)\n\n        super(JupyterOperator, self).__init__(bash_command = command, *args, **kwargs)\n\n\nExample Usage\nSee the Juypter Notebook Task Definition File example in the gusty demo, in the stock_predictions DAG. Notice how the first cell in the notebook is a YAML cell (see Raw notebook)."
  },
  {
    "objectID": "create-dag-args.html#gusty-specific-arguments",
    "href": "create-dag-args.html#gusty-specific-arguments",
    "title": "Appendix A — create_dag Arguments",
    "section": "A.1 gusty-specific arguments",
    "text": "A.1 gusty-specific arguments\n\nlatest_only\nBy default, gusty adds a LatestOnlyOperator at the absolute root of your Airflow DAG, which means that - by default - the tasks is your DAG will not run except for the latest DAG run. You can read more about the LatestOnlyOperator in Airflow’s documentation, but setting latest_only=False will ensure a gusty-generated DAG mirrors Airflow’s default behavior.\n\n\nextra_tags\nIn addition to any tags set via an Airflow DAG’s tags argument (available - as with any Airflow DAG parameter - in both create_dag and METADATA.yml), gusty will append any tags set in the extra_tags list to the provided tags.\nTo set extra_tags in your call to create_dag, provide a list like so:\nextra_tags=[\"your\", \"extra\", \"tags\"]\n\n\nroot_tasks\nYou can assign certain tasks to be at the beginning of the DAG by declaring root_tasks, a list of task ids. Any task id that is designated as a root task cannot have a dependencies block.\n\n\nleaf_tasks\nYou can assign certain tasks to be at the end of the DAG by declaring leaf_tasks, a list of task ids. Any task id that is designated as a leaf task cannot have a dependencies block.\n\n\nexternal_dependencies\nA list of key value pairs in the format of dag_id: task_id, where the dag_id is some upstream DAG and the task_id is the task in that upstream DAG. When set, gusty will create ExternalTaskSensor tasks and place them at the root of the DAG. Set the task_id to all to wait for the entire upstream DAG to complete. See the section on external dependencies for more details.\n\n\ndag_constructors\nProvide either a list of functions or a dictionary of function names names and functions (much like what you would pass to an Airflow DAG’s user_defined_macros) to have your functions available to you both as YAML constructors with gusty as well as in Airflow anywhere user_defined_macros are accepted.\ngusty will consolidate your user_defined_macros and your dag_constructors so that all are available anywhere you’d expect. Really, you can just use the Airflow DAG object’s user_defined_macros for everything.\n\nlist format\nThe list format for dag_constructors would look like this:\ndag_constructors=[your_first_func, your_second_func]\nThe functions would be accessible based on their function name.\n\n\ndictionary format\nThe dictionary format for dag_constructors would look like this:\ndag_constructors={\n  \"your_first_func\": your_first_func,\n  \"your_renamed_func\": your_second_func\n  }\nThe functions would be accessible by the key name, allowing you to - as illustrated above - renamed your functions if you so desire.\nAgain, you can just use Airflow’s built-in user_defined_macros argument to achieve this same functionality, of having your macros available to you anywhere.\n\n\n\nwait_for_defaults\nA dictionary of values that can be passed to an Airflow ExternalTaskSensor (or BaseOperator).\n\n\ntask_group_defaults\nA dictionary of values that can be passed to Airflow TaskGroup object.\n\n\nleaf_tasks_from_dict\nA dictionary of tasks that you want at the end of your DAG, where the key is the name of the task, and the value is a spec for that task.\nleaf_tasks_from_dict={\n  \"my_dag_is_done\": {\n    \"operator\": \"airflow.operators.bash.BashOperator\",\n    \"bash_command\": \"echo done\"\n    }\n}\n\n\nparse_hooks\nIf you want to parse another file type, or want to override how gusty parses supported file types, you can pass a dictionary of file extensions and functions to parse those extensions. Your functions should take a file_path argument.\nparse_hooks={\n  \".sh\": your_shell_file_parsing_function\n}\nSee gusty’s built-in parsers here.\n\n\nignore_subfolders\nWill disable the creation of task groups from subfolders when set to True.\n\n\nrender_on_create\nDisabled by default. If you want any Jinja in your spec to rendered on creation, set to True. Note that this will process everything every time the DAG is processed, which by default in Airflow is every few minutes. In general you don’t want this on."
  },
  {
    "objectID": "create-dag-args.html#create_dag-specific-notes",
    "href": "create-dag-args.html#create_dag-specific-notes",
    "title": "Appendix A — create_dag Arguments",
    "section": "A.2 create_dag Specific Notes",
    "text": "A.2 create_dag Specific Notes\nThe first argument to create_dag is a path to single DAG directory containing Task Definition Files."
  },
  {
    "objectID": "create-dag-args.html#create_dags-specific-notes",
    "href": "create-dag-args.html#create_dags-specific-notes",
    "title": "Appendix A — create_dag Arguments",
    "section": "A.3 create_dags Specific Notes",
    "text": "A.3 create_dags Specific Notes\nThe first argument to create_dags is a path to a directory containing multiple DAG directories, each with their own Task Definition Files.\nThe second argument to create_dag should always be globals(), which will ensure the resulting DAG objects are discoverable by Airflow."
  }
]